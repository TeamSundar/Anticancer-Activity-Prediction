from numpy.random import seed
seed(1)
#from tensorflow import set_random_seed

#tf.random.set_seed(seed)
#set_random_seed(2)

import tensorflow
tensorflow.random.set_seed(2)



import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'


import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import AdaBoostClassifier

#from lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction
#from grid_search_utils import plot_grid_search, table_grid_search

import pickle




from pathlib import Path
from warnings import filterwarnings



import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import Draw, PandasTools
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import OneHotEncoder
from tensorflow import keras
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.preprocessing.text import Tokenizer
import matplotlib.pyplot as plt

# Silence some expected warnings
filterwarnings("ignore")

np.random.seed(1000)



df = pd.read_csv("/home/titan-4/NCI_60_activity.csv", sep=',')


print(f"Shape of dataframe: {df.shape}\n")
# NBVAL_CHECK_OUTPUT


def assess_two_letter_elements(df):
    """
    Find the two letter elements in dataframe.

    Parameters
    ----------
    df : pandas.DataFrame
        Dataframe which requires preprocessing.

    Returns
    -------
    two_letter_elements : list
        List with found two letter elements
    """

    # Search for unique characters in SMILES strings
    unique_chars = set(df.smiles.apply(list).sum())
    # Get upper and lower case letters only
    upper_chars = []
    lower_chars = []
    for entry in unique_chars:
        if entry.isalpha():
            if entry.isupper():
                upper_chars.append(entry)
            elif entry.islower():
                lower_chars.append(entry)
    #print(f"Upper letter characters {sorted(upper_chars)}")
    #print(f"Lower letter characters {sorted(lower_chars)}")

    # List of all possible periodic elements
    periodic_elements = [
        "Ac",
        "Al",
        "Am",
        "Sb",
        "Ar",
        "As",
        "At",
        "Ba",
        "Bk",
        "Be",
        "Bi",
        "Bh",
        "B",
        "Br",
        "Cd",
        "Ca",
        "Cf",
        "C",
        "Ce",
        "Cs",
        "Cl",
        "Cr",
        "Co",
        "Cn",
        "Cu",
        "Cm",
        "Ds",
        "Db",
        "Dy",
        "Es",
        "Er",
        "Eu",
        "Fm",
        "Fl",
        "F",
        "Fr",
        "Gd",
        "Ga",
        "Ge",
        "Au",
        "Hf",
        "Hs",
        "He",
        "Ho",
        "H",
        "In",
        "I",
        "Ir",
        "Fe",
        "Kr",
        "La",
        "Lr",
        "Pb",
        "Li",
        "Lv",
        "Lu",
        "Mg",
        "Mn",
        "Mt",
        "Md",
        "Hg",
        "Mo",
        "Mc",
        "Nd",
        "Ne",
        "Np",
        "Ni",
        "Nh",
        "Nb",
        "N",
        "No",
        "Og",
        "Os",
        "O",
        "Pd",
        "P",
        "Pt",
        "Pu",
        "Po",
        "K",
        "Pr",
        "Pm",
        "Pa",
        "Ra",
        "Rn",
        "Re",
        "Rh",
        "Rg",
        "Rb",
        "Ru",
        "Rf",
        "Sm",
        "Sc",
        "Sg",
        "Se",
        "Si",
        "Ag",
        "Na",
        "Sr",
        "S",
        "Ta",
        "Tc",
        "Te",
        "Ts",
        "Tb",
        "Tl",
        "Th",
        "Tm",
        "Sn",
        "Ti",
        "W",
        "U",
        "V",
        "Xe",
        "Yb",
        "Y",
        "Zn",
        "Zr",
    ]

    # The two_char_elements list contains all two letter elements
    # which can be generated by all possible combination of upper x lower characters
    # and are valid periodic elements.
    two_char_elements = []
    for upper in upper_chars:
        for lower in lower_chars:
            ch = upper + lower
            if ch in periodic_elements:
                two_char_elements.append(ch)

    # This list is then reduced to the subset of two-letter elements
    # that actually appear in the SMILES strings, specific to our data set.
    two_char_elements_smiles = set()
    for char in two_char_elements:
        if df.smiles.str.contains(char).any():
            two_char_elements_smiles.add(char)

    return two_char_elements_smiles



elements_found = assess_two_letter_elements(df)
#print(f"\nTwo letter elements found in the data set: {sorted(elements_found)}")
# NBVAL_CHECK_OUTPUT

replace_dict = {"Ac" : "J", "Ag" : "Q", "As" : "X", "Au" : "j", "Ba" : "k", "Bi" : "p", "Br" : "q", "Ca" : "v", "Cd" : "w", "Ce" : "x", "Cl" : "z", "Co" : "α", "Cr" : "β", "Cu" : "γ", "Dy" : "δ", "Er" : "ε", "Eu" : "ζ", "Fe" : "η", "Ga" : "θ","Gd" : "ι", "Ge" :"κ", "Hf" : "λ", "Hg" : "μ", "In" :"ν", "Ir" : "ξ", "La" : "é", "Li" : "π", "Mg" : "ρ", "Mn" : "σ", "Mo" : "τ", "Na" : "υ", "Nb" : "φ", "Nd" : "χ", "Ni" : "ψ", "Os" : "ω", "Pb" : "ς", "Pd" :	"Γ", "Pt" : "Θ", "Re" :	"Ξ","Rh" : "Π", "Ru" : "Σ", "Sb" : "Φ", "Se" : "Ψ", "Si" : "Ω", "Sm" : "Á", "Sn" : "É", "Ta" : "Í", "Te" : "Ó", "Th" : "Ö", "Ti" : "Ő", "Tl" : "Ú", "Zn" : "Ü", "Zr" : "Ű"}


def preprocessing_data(df, replacement):
    """
    Preprocess the SMILES structures in a data set.

    Parameters
    ----------
    df : pandas.DataFrame
      Dataframe which requires preprocessing.
    replacement : dict
      Dictionary with mapping for replacement.

    Returns
    -------
    df : pandas.DataFrame
        Dataframe with new processed canonical SMILES column.
    unique_char : list
        List with unique characters present in SMILES.
    """
    # Print warning if the data set has a 'Sc' element
    if df.smiles.str.contains("Sc").any():
        print(
            'Warning: "Sc" element is found in the data set, since the element is rarely found '
            "in the drugs so we are not converting  "
            'it to single letter element, instead considering "S" '
            'and "c" as separate elements. '
        )

    # Create a new column having processed canonical SMILES
    df["processed_smiles"] = df["smiles"].copy()

    # Replace the two letter elements found with one character
    for pattern, repl in replacement.items():
        df["processed_smiles"] = df["processed_smiles"].str.replace(
            pattern, repl
        )

    unique_char = set(df.processed_smiles.apply(list).sum())
    return df, unique_char
    
# Calling function
df, unique_char = preprocessing_data(df, replace_dict)
#df.head(3)

# Print unique characters
#print(f"All unique characters found in the preprocessed data set:\n{sorted(unique_char)}")
# NBVAL_CHECK_OUTPUT

# Index of the longest SMILES string
longest_smiles = max(df["smiles"], key=len)
longest_smiles_index = df.smiles[df.smiles == longest_smiles].index.tolist()
#print(f"Longest SMILES: {longest_smiles}")
#print(f"Contains {len(longest_smiles)} characters, index in dataframe: {longest_smiles_index[0]}.")
smiles_maxlen = len(longest_smiles)
# NBVAL_CHECK_OUTPUT

# Function defined to create one-hot encoded matrix
def smiles_encoder(smiles, max_len, unique_char):
    """
    Function defined using all unique characters in our
    processed canonical SMILES structures created
    with the preprocessing_data function.

    Parameters
    ----------
    smiles : str
         SMILES of a molecule in string format.
    unique_char : list
         List of unique characters in the string data set.
    max_len : int
         Maximum length of the SMILES string.

    Returns
    -------
    smiles_matrix : numpy.ndarray
         One-hot encoded matrix of fixed shape
         (unique char in smiles, max SMILES length).
    """
    # create dictionary of the unique char data set
    smi2index = {char: index for index, char in enumerate(unique_char)}
    # one-hot encoding
    # zero padding to max_len
    smiles_matrix = np.zeros((len(unique_char), max_len))
    for index, char in enumerate(smiles):
        smiles_matrix[smi2index[char], index] = 1
    return smiles_matrix
    
    
# Apply the function to the processed canonical SMILES strings
df["unique_char_ohe_matrix"] = df["processed_smiles"].apply(
    smiles_encoder, max_len=smiles_maxlen, unique_char=unique_char
)
#df.head(3)



#df = df.fillna(0)
#df.info()


df=df[['unique_char_ohe_matrix', 'Activity']]


#df.info()


#df.shape


X = df[['unique_char_ohe_matrix','Activity']]

X=df.dropna()

#X.info()

#X

X.columns = ['feature_N' + str(i + 1) for i in range(X.shape[1])]
x = X['feature_N1'].explode().to_frame()
x['observation_id'] = x.groupby(level=0).cumcount()
x = x.pivot(columns='observation_id', values='feature_N1').fillna(0)
x = x.add_prefix('list_elementN_')
X.drop(columns='feature_N1', axis=1, inplace=True)
X = pd.concat([X, x], axis=1)
#X.info()
#X[["list_elementN_0"]]
X = X[['list_elementN_0','feature_N2']]
X = pd.concat([X.pop('list_elementN_0').apply(pd.Series), X['feature_N2']], axis=1)

#X

feature = X.drop('feature_N2', axis=1)
target = X['feature_N2']
#print(target)


X1=feature
y1=target




#np.random.seed(42)


# Train an Adaboost Classifier on the training data
from sklearn.ensemble import AdaBoostClassifier

# Implementing Oversampling for Handling Imbalanced 
from imblearn.combine import SMOTETomek
np.random.seed(1000)

smk = SMOTETomek(random_state=42)
X_res1,y_res1=smk.fit_resample(X1,y1)
X_res1.shape,y_res1.shape


from collections import Counter
print('Original dataset shape {}'.format(Counter(y1)))
print('Resampled dataset shape {}'.format(Counter(y_res1)))

from sklearn.model_selection import train_test_split
X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X_res1,y_res1, test_size=0.20, random_state=42)

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
scaler.fit(X_train1)
X_train1 = scaler.transform(X_train1)

scaler = MinMaxScaler()
scaler.fit(X_test1)
X_test2 = scaler.transform(X_test1)



ab_clf = AdaBoostClassifier(n_estimators=100, random_state=42)
#clf.fit(X_train1, Y_train1)

parameters = {
	    'n_estimators': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 20],
	    'learning_rate': [(0.97 + x / 100) for x in range(0, 8)],
	    'algorithm': ['SAMME', 'SAMME.R']
}
clf = GridSearchCV(ab_clf, parameters, cv=5, verbose=0, n_jobs=-1)
clf.fit(X_train1, Y_train1)
#plot_grid_search(clf)
#table_grid_search(clf)


clf.best_estimator_











    

    

    
    
    



